{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","import gc\n","import cv2 as cv\n","import cv2\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","from PIL import Image\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["cats = os.listdir('./fruit_vegetable/train/')\n","path = './fruit_vegetable/'"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["36"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["len(cats)"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["def extract_data(dataset_name,path_name):\n","    path = path_name\n","    data=[]\n","    im_w = 224\n","    im_h = 224\n","    for x in range(len(cats)):\n","        sub_path = path + dataset_name + '/' + cats[x] + '/'\n","        for y in os.listdir(sub_path):        \n","            img_path = sub_path + y  \n","            last = img_path[-12:]\n","            imag = cv2.imread(img_path)  \n","            if last == 'Image_56.jpg':\n","                continue\n","            if last == 'Image_96.jpg': \n","                continue\n","            img_from_ar = Image.fromarray(imag, 'RGB')\n","            resized_image = img_from_ar.resize((im_w, im_h))\n","            data.append([np.array(resized_image),x])\n","    return data"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["train = extract_data('train', path)"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["test = extract_data('test', path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","net = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained = True)\n","for params in net.parameters():\n","    params.requires_grad=False\n","net.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["net.classifier[-3] = torch.nn.Linear(in_features=4096, out_features=4096, bias=True)\n","net.classifier[-1] = torch.nn.Linear(in_features=4096, out_features=1000, bias=True)\n","net = torch.nn.Sequential(net,torch.nn.Linear(in_features=1000,out_features=36,bias=True),torch.nn.Softmax(dim=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torchvision.transforms as transforms\n","\n","# Normalize training set together with augmentation\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n","])\n","\n","# Normalize test set same as training set without augmentation\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n","])\n","\n","batch_size=16"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class ImageDataset(Dataset):\n","    \"\"\"Face Landmarks dataset.\"\"\"\n","    def __init__(self,dataset):\n","        self.dataset = dataset\n","    def __len__(self):\n","        return len(self.dataset)\n","    def __getitem__(self, idx):\n","        return self.dataset[idx]\n","\n","trainset = ImageDataset(train)\n","testset = ImageDataset(test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","def generate_batch(batch):\n","    images = [transform(torch.from_numpy(x[0])) for x in batch]\n","    label = [x[1] for x in batch]\n","    images = [t.numpy() for t in images]\n","    images = torch.Tensor(images)\n","    label = torch.Tensor(label)\n","    label = label.to(dtype=torch.long)\n","    return images,label\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2,collate_fn = generate_batch)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2,collate_fn = generate_batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch.optim as optim\n","import torch.nn as nn\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(),lr = 0.001,momentum = 0.9)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","net.to(device)\n","criterion.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","train_losses = []\n","validation_losses = []\n","for epoch in range(25):  # loop over the dataset multiple times\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        loss = criterion(outputs,labels)\n","        loss.backward()\n","        optimizer.step()\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 2000 == 1999:    # print every 2000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 2000))\n","            running_loss = 0.0\n","    train_losses.append((running_loss*32)/len(trainloader))\n","    running_loss = 0.0\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data\n","            images,labels = images.to(device),labels.to(device)\n","            outputs = net(images)\n","            loss = criterion(outputs,labels)\n","            running_loss += loss.item()\n","    validation_losses.append((running_loss*32)/len(testloader))\n","    print(\"{} Epoch done\".format(epoch))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('Finished Training')\n","plt.plot(train_losses,label = 'train')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.plot(validation_losses,label = 'validation')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["PATH = './alexnet_fine_tuned.pth'\n","torch.save(net.state_dict(), PATH)\n","temp_net = net"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["PATH = '../input/alexnet-fined-tuned/alexnet_fine_tuned.pth'\n","# net = Net()\n","net.load_state_dict(torch.load(PATH))\n","temp_net = net"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["net = net[:-2]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def make_data(dataloader):\n","    train_features,train_labels = [],[]\n","    with torch.no_grad():\n","        for data in dataloader:\n","            images, labels = data\n","            images,labels = images.to(device),labels.to(device)\n","            output = net(images)\n","            train_features.extend(output)\n","            train_labels.extend(labels)\n","    return train_features,train_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device1 = torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_features,train_labels = make_data(trainloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_features,test_labels = make_data(testloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["training_data = [t.to(device1).numpy() for t in train_features]\n","labels = [t.to(device1).numpy() for t in train_labels]\n","testing_data = [t.to(device1).numpy() for t in train_features]\n","test_labels = [t.to(device1).numpy() for t in train_labels]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["training_data,labels = pd.DataFrame(training_data),pd.DataFrame(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["testing_data,testing_labels = pd.DataFrame(testing_data),pd.DataFrame(test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["training_data.to_csv('./training_data.csv',index=False)\n","labels.to_csv('./labels.csv',index=False)\n","testing_data.to_csv('./testing_data.csv',index=False)\n","testing_labels.to_csv('./test_labels.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train = pd.concat([training_data,testing_data])\n","y = pd.concat([labels,testing_labels])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train,X_test,y_train,y_test = train_test_split(train,y,shuffle=True,random_state=2021,test_size=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.svm import SVC\n","\n","svc = SVC(kernel='linear')\n","svc.fit(X_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","y_pred = svc.predict(X_test)\n","print(accuracy_score(y_test,y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","\n","lr = LogisticRegression(max_iter = 1000,l1_ratio=0)\n","lr.fit(X_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","y_pred = lr.predict(X_test)\n","print(accuracy_score(y_test,y_pred))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}
